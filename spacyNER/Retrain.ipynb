{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals,print_function\n",
    "\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "import spacy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch,compounding\n",
    "nlp_de1 = spacy.load(\"de_core_news_sm\")\n",
    "nlp_de2 = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frankreich.', ['Frankreich', 0, 10, 'loc']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"novo_train_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Frankreich.', {'entities': [(0, 10, 'LOC')]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_format_for_train(data):    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(1,len(data[i])):\n",
    "            tmp = data[i][j][-1]\n",
    "            if tmp == 'pers':\n",
    "                data[i][j][-1]=\"PER\"\n",
    "            elif tmp == 'org':\n",
    "                data[i][j][-1]='ORG'\n",
    "            elif tmp == 'loc':\n",
    "                data[i][j][-1]='LOC'\n",
    "            else:\n",
    "                data[i][j][-1]='MISC' \n",
    "    DATA = []\n",
    "    for i in range(len(data)):\n",
    "        values = [(x[1],x[2],x[3]) for x in data[i][1:]]\n",
    "        DATA.append((data[i][0],{\"entities\":values}))\n",
    "    return DATA\n",
    "TRAIN_DATA = spacy_format_for_train(train_data)\n",
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses, {'ner': 21901.252704107843}\n",
      "Losses, {'ner': 5575.999694520142}\n",
      "Losses, {'ner': 5339.034751672356}\n",
      "Losses, {'ner': 4942.349053597078}\n",
      "Losses, {'ner': 4680.652463650156}\n",
      "Losses, {'ner': 4660.858366197004}\n",
      "Losses, {'ner': 4230.358784696698}\n",
      "Losses, {'ner': 4125.303089174908}\n",
      "Losses, {'ner': 3803.2817659351276}\n",
      "Losses, {'ner': 3809.411183443386}\n",
      "Losses, {'ner': 3493.343972873814}\n",
      "Losses, {'ner': 3603.288161922901}\n",
      "Losses, {'ner': 3420.027510996748}\n",
      "Losses, {'ner': 3299.4164641096067}\n",
      "Losses, {'ner': 3260.965101563066}\n",
      "Losses, {'ner': 3103.8086719831554}\n",
      "Losses, {'ner': 3102.76538397102}\n",
      "Losses, {'ner': 2945.934652855336}\n",
      "Losses, {'ner': 2729.9249583415403}\n",
      "Losses, {'ner': 3000.407493845226}\n",
      "Losses, {'ner': 2719.1636139813672}\n",
      "Losses, {'ner': 2567.027094748677}\n",
      "Losses, {'ner': 2579.592337316328}\n",
      "Losses, {'ner': 2529.518233144954}\n",
      "Losses, {'ner': 2421.1188388117816}\n",
      "Losses, {'ner': 2635.7878836361465}\n",
      "Losses, {'ner': 2338.217444127071}\n",
      "Losses, {'ner': 2348.42946402892}\n",
      "Losses, {'ner': 2155.1328641731366}\n",
      "Losses, {'ner': 2345.9553509682855}\n",
      "Losses, {'ner': 2024.0991188853263}\n",
      "Losses, {'ner': 2039.3293948910746}\n",
      "Losses, {'ner': 2055.3778795956337}\n",
      "Losses, {'ner': 2009.7534191298066}\n",
      "Losses, {'ner': 1832.4508412399082}\n",
      "Losses, {'ner': 1810.663912651171}\n",
      "Losses, {'ner': 2080.882484433571}\n",
      "Losses, {'ner': 1713.1587957590987}\n",
      "Losses, {'ner': 1670.0312964040268}\n",
      "Losses, {'ner': 1650.462245956765}\n",
      "Losses, {'ner': 1542.3936202035557}\n",
      "Losses, {'ner': 1488.6856058480244}\n",
      "Losses, {'ner': 1427.2216885268674}\n",
      "Losses, {'ner': 1476.5317635785627}\n",
      "Losses, {'ner': 1347.0567960737653}\n",
      "Losses, {'ner': 1390.1639847096833}\n",
      "Losses, {'ner': 1256.2252624117477}\n",
      "Losses, {'ner': 1241.2312478029216}\n",
      "Losses, {'ner': 1222.3505397954784}\n",
      "Losses, {'ner': 1200.4871193167946}\n",
      "Losses, {'ner': 1207.8744752972755}\n",
      "Losses, {'ner': 1251.1822981586606}\n",
      "Losses, {'ner': 1158.6521507179534}\n",
      "Losses, {'ner': 1121.4706864444001}\n",
      "Losses, {'ner': 1058.265087748715}\n",
      "Losses, {'ner': 1069.2318073959277}\n",
      "Losses, {'ner': 1039.7791164363348}\n",
      "Losses, {'ner': 1079.2166871329468}\n",
      "Losses, {'ner': 1129.4254070741954}\n",
      "Losses, {'ner': 1091.5723116127936}\n",
      "Losses, {'ner': 957.8638754336364}\n",
      "Losses, {'ner': 960.074616814145}\n",
      "Losses, {'ner': 879.3361853242712}\n",
      "Losses, {'ner': 890.8429293587451}\n",
      "Losses, {'ner': 913.7527836924485}\n",
      "Losses, {'ner': 863.4118464010517}\n",
      "Losses, {'ner': 798.9957643652394}\n",
      "Losses, {'ner': 854.5483532262182}\n",
      "Losses, {'ner': 835.6574901357717}\n",
      "Losses, {'ner': 776.121276619108}\n",
      "Losses, {'ner': 766.2581222343598}\n",
      "Losses, {'ner': 737.8265279727364}\n",
      "Losses, {'ner': 772.9373772324789}\n",
      "Losses, {'ner': 752.987630528976}\n",
      "Losses, {'ner': 715.3303961047851}\n",
      "Losses, {'ner': 662.7815802491539}\n",
      "Losses, {'ner': 700.236708130139}\n",
      "Losses, {'ner': 752.1726764748256}\n",
      "Losses, {'ner': 710.7095921385434}\n",
      "Losses, {'ner': 648.7923905239717}\n",
      "Losses, {'ner': 705.7099434265696}\n",
      "Losses, {'ner': 681.5635727503457}\n",
      "Losses, {'ner': 702.0967778921065}\n",
      "Losses, {'ner': 819.0567729172928}\n",
      "Losses, {'ner': 766.9633737528316}\n",
      "Losses, {'ner': 647.647320847943}\n",
      "Losses, {'ner': 680.6306494977144}\n",
      "Losses, {'ner': 648.7753535348479}\n",
      "Losses, {'ner': 794.7536572974859}\n",
      "Losses, {'ner': 826.1138640629334}\n",
      "Losses, {'ner': 657.5533676546604}\n",
      "Losses, {'ner': 608.7915463489768}\n",
      "Losses, {'ner': 602.6585624665399}\n",
      "Losses, {'ner': 623.0189360643386}\n",
      "Losses, {'ner': 581.759848145101}\n",
      "Losses, {'ner': 546.3832669426423}\n",
      "Losses, {'ner': 565.9627530247599}\n",
      "Losses, {'ner': 569.0821737392862}\n",
      "Losses, {'ner': 533.8109390591317}\n",
      "Losses, {'ner': 540.3698442758587}\n",
      "Losses, {'ner': 524.9270747152497}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def create_model(output_dir,n_iter,TRAIN_DATA):\n",
    "    nlp = spacy.blank(\"de\")\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner,last=True)\n",
    "    \n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch ï¼ˆspeed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "create_model(\"/home/zijian/ZijianStageNER/Novo_Models/Create/\",101,TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch ï¼ˆspeed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this new model with dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"novo_dev_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "\n",
    "DEV_DATA = spacy_format_for_train(dev)\n",
    "for i in range(len(DEV_DATA)):\n",
    "    DEV_DATA[i] = (DEV_DATA[i][0],DEV_DATA[i][1][\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(Condeer.] Petersburg den 18. Dec. Se.russisch- kaiserl. MajestÃ¤t haben dem Prinzen vonCondÃ© bey dessen Ankunft in Petersburg den St.Andreas-Orden u. den Maltheser Ritterorden in Polen zu ertheilen, und ihn mit einem prÃ¤chtigen,vÃ¶llig meublirten Palais in Petersburg zu beschenken geruhet. Das aus 3 Infanterie- und 2 Kavallerie-Regimentern bestehende Corps des Prinzen vonCondÃ©, welches in kaiserliche Dienste genommenworden, ist nun nach Wladimir, Luzk und Kowelin Quartier verlegt. Das ganze Corps wird unterbestandiger Inspection des Prinzen von CondÃ©stehen. Se. kaiserl. MajestÃ¤t haben ihn zum Chef desadelichen Infanterie-Regiments, und den Duc deBerry zum Chef des adelichen Kavallerie-Regiments ernannt. Als der Prinz in seinen Pallasttrat, fand er daselbst bereits Leute mit seiner LibrÃ©evor, auch Carossen mit seinem Wappen. Der Prinzwar in Verlegenheit an welcher Stelle er eigentlichdas Zeichen des St. Andreas-Ordens tragen sollte.Der Kaiser antwortete ihm: Er mÃ¶chte es mit denInsignien des hl. Geist-Ordens auf derselben Linietragen, und hieng ihm den Orden um.',\n",
       " [(11, 21, 'LOC'),\n",
       "  (76, 83, 'PER'),\n",
       "  (115, 125, 'LOC'),\n",
       "  (179, 184, 'LOC'),\n",
       "  (256, 266, 'LOC'),\n",
       "  (362, 369, 'PER'),\n",
       "  (440, 448, 'LOC'),\n",
       "  (450, 454, 'LOC'),\n",
       "  (459, 466, 'LOC'),\n",
       "  (538, 545, 'PER'),\n",
       "  (647, 650, 'PER')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model,data):\n",
    "    scorer = Scorer()\n",
    "    for text,annot in data:\n",
    "        doc_gold_text = ner_model.make_doc(text)\n",
    "        gold = GoldParse(doc_gold_text,entities=annot)\n",
    "        pred_value = ner_model(text)\n",
    "        scorer.score(pred_value,gold)\n",
    "    return scorer.scores\n",
    "\n",
    "\n",
    "path = \"/home/zijian/ZijianStageNER/Novo_Models/Create\"\n",
    "nlp2 = spacy.load(path)\n",
    "results = evaluate(nlp2,DEV_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train by updating the model in spacy de_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch ï¼ˆspeed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precison</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model only with my train_data</th>\n",
       "      <td>64.814815</td>\n",
       "      <td>53.030303</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model with data TigerCorpus and my train data</th>\n",
       "      <td>70.909091</td>\n",
       "      <td>59.090909</td>\n",
       "      <td>64.462810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                precison     recall         f1\n",
       "Model only with my train_data                  64.814815  53.030303  58.333333\n",
       "Model with data TigerCorpus and my train data  70.909091  59.090909  64.462810"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/zijian/ZijianStageNER/RetrainModels/\"\n",
    "nlp2 = spacy.load(path)\n",
    "results2 = evaluate(nlp2,DEV_DATA)\n",
    "\n",
    "res = pd.DataFrame({\"precison\":[results['ents_p'],results2['ents_p']],\"recall\":[results['ents_r'],results2['ents_r']],\"f1\":[results['ents_f'],results2['ents_f']]},index=[\"Model only with my train_data\",\"Model with data TigerCorpus and my train data\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What is the difference? Model 1 could detect what? Model 2 could detect what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data_Model = spacy.load(\"/home/zijian/ZijianStageNER/Novo_Models/Create/\")\n",
    "SM_plus_Train_DATA_Model = spacy.load(\"/home/zijian/ZijianStageNER/RetrainModels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trouvÃ©s_corrects = []\n",
    "trouvÃ©s_incorrect = []\n",
    "non_trouvÃ©_mais_correct = []\n",
    "res1 = []\n",
    "correction1 = []\n",
    "for i in range(len(dev)):\n",
    "    doc = SM_plus_Train_DATA_Model(dev[i][0])\n",
    "    correction1.append(dev[i][1:])\n",
    "    tmp = []\n",
    "    for ent in doc.ents:\n",
    "        tmp.append([ent.text,ent.start_char,ent.end_char,ent.label_])\n",
    "    res1.append(tmp)\n",
    "for i in range(len(res1)):\n",
    "    l1 = [(x[0],x[1],x[2],x[3]) for x in res1[i]]\n",
    "    l2 = [(x[0],x[1],x[2],x[3]) for x in correction1[i]]\n",
    "    trouvÃ©s_corrects += list(set(l1) & set(l2))\n",
    "    trouvÃ©s_incorrect += list(set(l1) - set(l2))\n",
    "    non_trouvÃ©_mais_correct += list(set(l2)-set(l1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(text,i,j):\n",
    "    left,right = i,j\n",
    "    count = 0\n",
    "    if left > 0:\n",
    "        while count < 5 and left > 0:\n",
    "            left-=1\n",
    "            if text[left]==' ':\n",
    "                count+=1\n",
    "    count = 0\n",
    "    while count < 5 and right < len(text):\n",
    "        right += 1\n",
    "        if text[right]==' ':\n",
    "            count += 1\n",
    "    return text[left+1:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RuÃŸland', 0, 7, 'LOC')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trouvÃ©s_corrects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp = max([len(trouvÃ©s_corrects),len(trouvÃ©s_incorrect),len(non_trouvÃ©_mais_correct)])\n",
    "for i in range(len(trouvÃ©s_corrects),tmp):\n",
    "    trouvÃ©s_corrects.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(trouvÃ©s_incorrect),tmp):\n",
    "    trouvÃ©s_incorrect.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(non_trouvÃ©_mais_correct),tmp):\n",
    "    non_trouvÃ©_mais_correct.append((\"\",\"\",\"\",\"\"))\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"TrouvÃ© corrects\":[(x[0],x[3]) for x in trouvÃ©s_corrects],\"TrouvÃ©s incorrects\":[(x[0],x[3]) for x in trouvÃ©s_incorrect],\"non trouvÃ© mais correct\":[(x[0],x[3]) for x in non_trouvÃ©_mais_correct]},index = [x for x in range(1,tmp+1)])\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Model only with train data</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "with open(\"Model with train data and spacy data.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string.format(table=d.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP and their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trouvÃ©s_incorrect = [(x[0],x[1],x[2],x[3],context(x[0],x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp = max([len(trouvÃ©s_corrects),len(trouvÃ©s_incorrect),len(non_trouvÃ©_mais_correct)])\n",
    "for i in range(len(trouvÃ©s_corrects),tmp):\n",
    "    trouvÃ©s_corrects.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(trouvÃ©s_incorrect),tmp):\n",
    "    trouvÃ©s_incorrect.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(non_trouvÃ©_mais_correct),tmp):\n",
    "    non_trouvÃ©_mais_correct.append((\"\",\"\",\"\",\"\"))\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"TrouvÃ© corrects\":[(x[0],x[3]) for x in trouvÃ©s_corrects],\"TrouvÃ©s incorrects\":[(x[0],x[3]) for x in trouvÃ©s_incorrect],\"non trouvÃ© mais correct\":[(x[0],x[3]) for x in non_trouvÃ©_mais_correct]},index = [x for x in range(1,tmp+1)])\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Model only with train data</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

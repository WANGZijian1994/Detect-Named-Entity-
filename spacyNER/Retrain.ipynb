{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals,print_function\n",
    "\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "import spacy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch,compounding\n",
    "nlp_de1 = spacy.load(\"de_core_news_sm\")\n",
    "nlp_de2 = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frankreich.', ['Frankreich', 0, 10, 'loc']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"novo_train_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Frankreich.', {'entities': [(0, 10, 'LOC')]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_format_for_train(data):    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(1,len(data[i])):\n",
    "            tmp = data[i][j][-1]\n",
    "            if tmp == 'pers':\n",
    "                data[i][j][-1]=\"PER\"\n",
    "            elif tmp == 'org':\n",
    "                data[i][j][-1]='ORG'\n",
    "            elif tmp == 'loc':\n",
    "                data[i][j][-1]='LOC'\n",
    "            else:\n",
    "                data[i][j][-1]='MISC' \n",
    "    DATA = []\n",
    "    for i in range(len(data)):\n",
    "        values = [(x[1],x[2],x[3]) for x in data[i][1:]]\n",
    "        DATA.append((data[i][0],{\"entities\":values}))\n",
    "    return DATA\n",
    "TRAIN_DATA = spacy_format_for_train(train_data)\n",
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses, {'ner': 21901.252704107843}\n",
      "Losses, {'ner': 5575.999694520142}\n",
      "Losses, {'ner': 5339.034751672356}\n",
      "Losses, {'ner': 4942.349053597078}\n",
      "Losses, {'ner': 4680.652463650156}\n",
      "Losses, {'ner': 4660.858366197004}\n",
      "Losses, {'ner': 4230.358784696698}\n",
      "Losses, {'ner': 4125.303089174908}\n",
      "Losses, {'ner': 3803.2817659351276}\n",
      "Losses, {'ner': 3809.411183443386}\n",
      "Losses, {'ner': 3493.343972873814}\n",
      "Losses, {'ner': 3603.288161922901}\n",
      "Losses, {'ner': 3420.027510996748}\n",
      "Losses, {'ner': 3299.4164641096067}\n",
      "Losses, {'ner': 3260.965101563066}\n",
      "Losses, {'ner': 3103.8086719831554}\n",
      "Losses, {'ner': 3102.76538397102}\n",
      "Losses, {'ner': 2945.934652855336}\n",
      "Losses, {'ner': 2729.9249583415403}\n",
      "Losses, {'ner': 3000.407493845226}\n",
      "Losses, {'ner': 2719.1636139813672}\n",
      "Losses, {'ner': 2567.027094748677}\n",
      "Losses, {'ner': 2579.592337316328}\n",
      "Losses, {'ner': 2529.518233144954}\n",
      "Losses, {'ner': 2421.1188388117816}\n",
      "Losses, {'ner': 2635.7878836361465}\n",
      "Losses, {'ner': 2338.217444127071}\n",
      "Losses, {'ner': 2348.42946402892}\n",
      "Losses, {'ner': 2155.1328641731366}\n",
      "Losses, {'ner': 2345.9553509682855}\n",
      "Losses, {'ner': 2024.0991188853263}\n",
      "Losses, {'ner': 2039.3293948910746}\n",
      "Losses, {'ner': 2055.3778795956337}\n",
      "Losses, {'ner': 2009.7534191298066}\n",
      "Losses, {'ner': 1832.4508412399082}\n",
      "Losses, {'ner': 1810.663912651171}\n",
      "Losses, {'ner': 2080.882484433571}\n",
      "Losses, {'ner': 1713.1587957590987}\n",
      "Losses, {'ner': 1670.0312964040268}\n",
      "Losses, {'ner': 1650.462245956765}\n",
      "Losses, {'ner': 1542.3936202035557}\n",
      "Losses, {'ner': 1488.6856058480244}\n",
      "Losses, {'ner': 1427.2216885268674}\n",
      "Losses, {'ner': 1476.5317635785627}\n",
      "Losses, {'ner': 1347.0567960737653}\n",
      "Losses, {'ner': 1390.1639847096833}\n",
      "Losses, {'ner': 1256.2252624117477}\n",
      "Losses, {'ner': 1241.2312478029216}\n",
      "Losses, {'ner': 1222.3505397954784}\n",
      "Losses, {'ner': 1200.4871193167946}\n",
      "Losses, {'ner': 1207.8744752972755}\n",
      "Losses, {'ner': 1251.1822981586606}\n",
      "Losses, {'ner': 1158.6521507179534}\n",
      "Losses, {'ner': 1121.4706864444001}\n",
      "Losses, {'ner': 1058.265087748715}\n",
      "Losses, {'ner': 1069.2318073959277}\n",
      "Losses, {'ner': 1039.7791164363348}\n",
      "Losses, {'ner': 1079.2166871329468}\n",
      "Losses, {'ner': 1129.4254070741954}\n",
      "Losses, {'ner': 1091.5723116127936}\n",
      "Losses, {'ner': 957.8638754336364}\n",
      "Losses, {'ner': 960.074616814145}\n",
      "Losses, {'ner': 879.3361853242712}\n",
      "Losses, {'ner': 890.8429293587451}\n",
      "Losses, {'ner': 913.7527836924485}\n",
      "Losses, {'ner': 863.4118464010517}\n",
      "Losses, {'ner': 798.9957643652394}\n",
      "Losses, {'ner': 854.5483532262182}\n",
      "Losses, {'ner': 835.6574901357717}\n",
      "Losses, {'ner': 776.121276619108}\n",
      "Losses, {'ner': 766.2581222343598}\n",
      "Losses, {'ner': 737.8265279727364}\n",
      "Losses, {'ner': 772.9373772324789}\n",
      "Losses, {'ner': 752.987630528976}\n",
      "Losses, {'ner': 715.3303961047851}\n",
      "Losses, {'ner': 662.7815802491539}\n",
      "Losses, {'ner': 700.236708130139}\n",
      "Losses, {'ner': 752.1726764748256}\n",
      "Losses, {'ner': 710.7095921385434}\n",
      "Losses, {'ner': 648.7923905239717}\n",
      "Losses, {'ner': 705.7099434265696}\n",
      "Losses, {'ner': 681.5635727503457}\n",
      "Losses, {'ner': 702.0967778921065}\n",
      "Losses, {'ner': 819.0567729172928}\n",
      "Losses, {'ner': 766.9633737528316}\n",
      "Losses, {'ner': 647.647320847943}\n",
      "Losses, {'ner': 680.6306494977144}\n",
      "Losses, {'ner': 648.7753535348479}\n",
      "Losses, {'ner': 794.7536572974859}\n",
      "Losses, {'ner': 826.1138640629334}\n",
      "Losses, {'ner': 657.5533676546604}\n",
      "Losses, {'ner': 608.7915463489768}\n",
      "Losses, {'ner': 602.6585624665399}\n",
      "Losses, {'ner': 623.0189360643386}\n",
      "Losses, {'ner': 581.759848145101}\n",
      "Losses, {'ner': 546.3832669426423}\n",
      "Losses, {'ner': 565.9627530247599}\n",
      "Losses, {'ner': 569.0821737392862}\n",
      "Losses, {'ner': 533.8109390591317}\n",
      "Losses, {'ner': 540.3698442758587}\n",
      "Losses, {'ner': 524.9270747152497}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def create_model(output_dir,n_iter,TRAIN_DATA):\n",
    "    nlp = spacy.blank(\"de\")\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner,last=True)\n",
    "    \n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch ï¼ˆspeed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "create_model(\"/home/zijian/ZijianStageNER/Novo_Models/Create/\",101,TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch ï¼ˆspeed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this new model with dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"novo_dev_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "\n",
    "DEV_DATA = spacy_format_for_train(dev)\n",
    "for i in range(len(DEV_DATA)):\n",
    "    DEV_DATA[i] = (DEV_DATA[i][0],DEV_DATA[i][1][\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(Condeer.] Petersburg den 18. Dec. Se.russisch- kaiserl. MajestÃ¤t haben dem Prinzen vonCondÃ© bey dessen Ankunft in Petersburg den St.Andreas-Orden u. den Maltheser Ritterorden in Polen zu ertheilen, und ihn mit einem prÃ¤chtigen,vÃ¶llig meublirten Palais in Petersburg zu beschenken geruhet. Das aus 3 Infanterie- und 2 Kavallerie-Regimentern bestehende Corps des Prinzen vonCondÃ©, welches in kaiserliche Dienste genommenworden, ist nun nach Wladimir, Luzk und Kowelin Quartier verlegt. Das ganze Corps wird unterbestandiger Inspection des Prinzen von CondÃ©stehen. Se. kaiserl. MajestÃ¤t haben ihn zum Chef desadelichen Infanterie-Regiments, und den Duc deBerry zum Chef des adelichen Kavallerie-Regiments ernannt. Als der Prinz in seinen Pallasttrat, fand er daselbst bereits Leute mit seiner LibrÃ©evor, auch Carossen mit seinem Wappen. Der Prinzwar in Verlegenheit an welcher Stelle er eigentlichdas Zeichen des St. Andreas-Ordens tragen sollte.Der Kaiser antwortete ihm: Er mÃ¶chte es mit denInsignien des hl. Geist-Ordens auf derselben Linietragen, und hieng ihm den Orden um.',\n",
       " [(11, 21, 'LOC'),\n",
       "  (76, 83, 'PER'),\n",
       "  (115, 125, 'LOC'),\n",
       "  (179, 184, 'LOC'),\n",
       "  (256, 266, 'LOC'),\n",
       "  (362, 369, 'PER'),\n",
       "  (440, 448, 'LOC'),\n",
       "  (450, 454, 'LOC'),\n",
       "  (459, 466, 'LOC'),\n",
       "  (538, 545, 'PER'),\n",
       "  (647, 650, 'PER')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model,data):\n",
    "    scorer = Scorer()\n",
    "    for text,annot in data:\n",
    "        doc_gold_text = ner_model.make_doc(text)\n",
    "        gold = GoldParse(doc_gold_text,entities=annot)\n",
    "        pred_value = ner_model(text)\n",
    "        scorer.score(pred_value,gold)\n",
    "    return scorer.scores\n",
    "\n",
    "\n",
    "#path = \"/home/zijian/ZijianStageNER/Novo_Models/Create\"\n",
    "#nlp2 = spacy.load(path)\n",
    "#results = evaluate(nlp2,DEV_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train by updating the model in spacy de_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch ï¼ˆspeed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precison</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model only with my train_data</th>\n",
       "      <td>64.814815</td>\n",
       "      <td>53.030303</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model with data TigerCorpus and my train data</th>\n",
       "      <td>70.909091</td>\n",
       "      <td>59.090909</td>\n",
       "      <td>64.462810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                precison     recall         f1\n",
       "Model only with my train_data                  64.814815  53.030303  58.333333\n",
       "Model with data TigerCorpus and my train data  70.909091  59.090909  64.462810"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/zijian/ZijianStageNER/RetrainModels/\"\n",
    "nlp2 = spacy.load(path)\n",
    "results2 = evaluate(nlp2,DEV_DATA)\n",
    "\n",
    "res = pd.DataFrame({\"precison\":[results['ents_p'],results2['ents_p']],\"recall\":[results['ents_r'],results2['ents_r']],\"f1\":[results['ents_f'],results2['ents_f']]},index=[\"Model only with my train_data\",\"Model with data TigerCorpus and my train data\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What is the difference? Model 1 could detect what? Model 2 could detect what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data_Model = spacy.load(\"ModelsRetrained/Novo_Models/Create/\")\n",
    "SM_plus_Train_DATA_Model = spacy.load(\"ModelsRetrained/Novo_Models/UpdateModel1/RetrainModels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40   40\n"
     ]
    }
   ],
   "source": [
    "def context(text,i,j):\n",
    "    left,right = i,j\n",
    "    count = 0\n",
    "    if left > 0:\n",
    "        while count < 5 and left > 0:\n",
    "            left-=1\n",
    "            if text[left]==' ':\n",
    "                count+=1\n",
    "    count = 0\n",
    "    while count < 5 and right+1 < len(text):\n",
    "        right += 1\n",
    "        if text[right]==' ':\n",
    "            count += 1\n",
    "    return text[left:right]\n",
    "\n",
    "def consulter(model,dev):\n",
    "    results = []\n",
    "    for i in range(len(dev)):\n",
    "        doc = model(dev[i][0])\n",
    "        tmp = []\n",
    "        for ent in doc.ents:\n",
    "            tmp.append([ent.text,ent.start_char,ent.end_char,ent.label_,context(dev[i][0],ent.start_char,ent.end_char)])\n",
    "        results.append(tmp) \n",
    "    return results\n",
    "\n",
    "# Model with train_de.json :\n",
    "res1 = consulter(Train_Data_Model,dev)\n",
    "\n",
    "# Model with train_de.json + sm:\n",
    "res2 = consulter(SM_plus_Train_DATA_Model,dev)\n",
    "\n",
    "print(len(res1),\" \",len(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Petersburg', 11, 21, 'LOC', '(Condeer.] Petersburg den 18. Dec. Se.russisch- kaiserl.'], ['Prinzen', 76, 83, 'PER', ' kaiserl. MajestÃ¤t haben dem Prinzen vonCondÃ© bey dessen Ankunft in'], ['Petersburg', 115, 125, 'LOC', ' bey dessen Ankunft in Petersburg den St.Andreas-Orden u. den Maltheser'], ['Polen', 179, 184, 'LOC', ' den Maltheser Ritterorden in Polen zu ertheilen, und ihn mit'], ['Petersburg', 256, 266, 'LOC', ' prÃ¤chtigen,vÃ¶llig meublirten Palais in Petersburg zu beschenken geruhet. Das aus'], ['Prinzen', 362, 369, 'PER', ' Kavallerie-Regimentern bestehende Corps des Prinzen vonCondÃ©, welches in kaiserliche Dienste'], ['Wladimir', 440, 448, 'LOC', ' genommenworden, ist nun nach Wladimir, Luzk und Kowelin Quartier'], ['Luzk', 450, 454, 'LOC', ' ist nun nach Wladimir, Luzk und Kowelin Quartier verlegt. Das'], ['Kowelin', 459, 466, 'LOC', ' nach Wladimir, Luzk und Kowelin Quartier verlegt. Das ganze Corps'], ['Prinzen', 538, 545, 'PER', ' wird unterbestandiger Inspection des Prinzen von CondÃ©stehen. Se. kaiserl. MajestÃ¤t'], ['Duc', 647, 650, 'PER', ' desadelichen Infanterie-Regiments, und den Duc deBerry zum Chef des adelichen']]\n"
     ]
    }
   ],
   "source": [
    "def correction():\n",
    "    correction = []\n",
    "    for i in range(len(dev)):\n",
    "        text = dev[i][0]\n",
    "        tmp = []\n",
    "        for x in dev[i][1:]:\n",
    "            tmp.append([x[0],x[1],x[2],x[3],context(text,x[1],x[2])])\n",
    "        correction.append(tmp[:])\n",
    "    return correction\n",
    "correction = correction()\n",
    "print(correction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(res,correction):        \n",
    "    VP = []\n",
    "    FP = []\n",
    "    VN = [] \n",
    "    i,j = 0,0\n",
    "    for x in range(len(res)):\n",
    "        a = res[x]\n",
    "        b = correction[x]\n",
    "        while i<len(a) and j<len(b):\n",
    "            l1,r1 = a[i][1],a[i][2]\n",
    "            l2,r2 = b[j][1],b[j][2]\n",
    "            if l1>r2:\n",
    "                VN.append(b[j])\n",
    "                j+=1\n",
    "            elif r1 < l2:\n",
    "                FP.append(a[i])\n",
    "                i+=1\n",
    "            elif l1==l2 and r1==r2:\n",
    "                if a[i][3]!=b[j][3]:\n",
    "                    a[i][3] += \" but correct : ({})\".format(b[j][3])\n",
    "                    FP.append(a[i])\n",
    "                else:\n",
    "                    VP.append(a[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "            else:\n",
    "                if a[i][0] in b[j][4]:\n",
    "                    if a[i][3]!=b[j][3]:\n",
    "                        a[i][3] += \" but correct : ({})\".format(b[j][3])\n",
    "                        FP.append(a[i])\n",
    "                    else:\n",
    "                        VP.append(a[i])\n",
    "                else:\n",
    "                    FP.append(a[i])\n",
    "                i+=1\n",
    "                j+=1  \n",
    "        while i<len(a):\n",
    "            FP.append(a[i])\n",
    "            i+=1\n",
    "        while j<len(b):\n",
    "            VN.append(b[j])\n",
    "            j+=1\n",
    "    return [VP,FP,VN]\n",
    "\n",
    "result1 = evaluate(res1,correction)\n",
    "result2 = evaluate(res2,correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d1 = pd.DataFrame({\"NER\":[x[0] for x in result1[1]],\"Type\":[x[3] for x in result1[1]],\"Context\":[x[4] for x in result1[1]]},index = [x for x in range(1,len(result1[1])+1)])\n",
    "d2 = pd.DataFrame({\"NER\":[x[0] for x in result1[2]],\"Type\":[x[3] for x in result1[2]],\"Context\":[x[4] for x in result1[2]]},index = [x for x in range(1,len(result1[2])+1)])\n",
    "\n",
    "d3 = pd.DataFrame({\"NER\":[x[0] for x in result2[1]],\"Type\":[x[3] for x in result2[1]],\"Context\":[x[4] for x in result2[1]]},index = [x for x in range(1,len(result2[1])+1)])\n",
    "d4 = pd.DataFrame({\"NER\":[x[0] for x in result2[2]],\"Type\":[x[3] for x in result2[2]],\"Context\":[x[4] for x in result2[2]]},index = [x for x in range(1,len(result2[2])+1)])\n",
    "\n",
    "\n",
    "html_string1 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Faux Postif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "html_string2 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Vrai NÃ©gatif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "with open(\"rÃ©sultat/Model only with train/Faux Positif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=d1.to_html()))\n",
    "with open(\"rÃ©sultat/Model only with train/Vrai NÃ©gatif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string2.format(table=d2.to_html()))\n",
    "with open(\"rÃ©sultat/Model with train + Spacy Model/Faux Positif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=d3.to_html()))\n",
    "with open(\"rÃ©sultat/Model with train + Spacy Model/Vrai NÃ©gatif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string2.format(table=d4.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for the results\n",
    "\n",
    "\n",
    "#### Type of NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(result2,typeLabel):\n",
    "    total = []\n",
    "    trouvÃ© = list(filter(lambda x:x[3]==typeLabel,result2[0]))\n",
    "    incorrect = list(filter(lambda x:x[3]==typeLabel,result2[1]))\n",
    "    non_trouvÃ© = list(filter(lambda x:x[3]==typeLabel,result2[2]))\n",
    "    total+=(trouvÃ©+incorrect+non_trouvÃ©)\n",
    "    p = len(trouvÃ©)/(len(trouvÃ©)+len(incorrect))*100\n",
    "    r = len(trouvÃ©)/(len(non_trouvÃ©)+len(trouvÃ©))*100\n",
    "    if p+r!=0:\n",
    "        f = 2*p*r/(p+r)\n",
    "    else:\n",
    "        f = 0.0\n",
    "    return [p,r,f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71.42857142857143, 48.38709677419355, 57.6923076923077]\n"
     ]
    }
   ],
   "source": [
    "loc = stats(result2,\"LOC\")\n",
    "print(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. PERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.83333333333333, 25.0, 32.35294117647059]\n"
     ]
    }
   ],
   "source": [
    "pers = stats(result2,\"PER\")\n",
    "print(pers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "orgs = stats(result2,\"ORG\")\n",
    "print(orgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.0, 9.523809523809524, 15.384615384615385]\n"
     ]
    }
   ],
   "source": [
    "misc = stats(result2,\"MISC\")\n",
    "print(misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miscellaneous entities, e.g. events, nationalities, products or works of art'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"MISC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PrÃ©cision</th>\n",
       "      <th>Rappel</th>\n",
       "      <th>F-Mesure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>71.428571</td>\n",
       "      <td>48.387097</td>\n",
       "      <td>57.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Person</th>\n",
       "      <td>45.833333</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>32.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organisation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miscellaneous entities</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>15.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        PrÃ©cision     Rappel   F-Mesure\n",
       "Location                71.428571  48.387097  57.692308\n",
       "Person                  45.833333  25.000000  32.352941\n",
       "Organisation             0.000000   0.000000   0.000000\n",
       "Miscellaneous entities  40.000000   9.523810  15.384615"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.DataFrame({\"PrÃ©cision\":[loc[0],pers[0],orgs[0],misc[0]],\"Rappel\":[loc[1],pers[1],orgs[1],misc[1]],'F-Mesure':[loc[2],pers[2],orgs[2],misc[2]]},index = [\"Location\",\"Person\",\"Organisation\",\"Miscellaneous entities\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string1 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Faux Postif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "html_string2 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Vrai NÃ©gatif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "with open(\"rÃ©sultat/Model with train + Spacy Model/Chaque_type.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=res.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe every NER not finded or finded but not correct\n",
    "\n",
    "##### Incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lausanne', 2), ('Petersburg', 1), ('St', 1), ('Kowelin', 1), ('Se', 1), ('Duc', 1), ('consolidirten3', 1), ('Landein', 1), ('Courier', 1), ('Lord', 1), ('KÃ¼hnheitgehabt', 1), ('LÃ¶wen', 1), ('Asselt', 1), ('Batavische', 1), ('Schuldenscheine', 1), ('Pest', 1), ('Zollvereinsstaatenzu', 1), ('Times', 1), ('Vizeadmiral', 1), ('Henry', 1), ('Talaat', 1), ('Volkan', 1), ('Graf', 1), ('Baron', 1), ('Ritter', 1), ('am', 1), ('Schweiz', 1), ('Rhodiaseta', 1), ('Celanaise', 1), ('British', 1), ('Bemberg', 1), ('Pemberg', 1), ('Kontinuierlich', 1), ('Eleinkeram', 1), ('Viscose', 1), ('Azetat', 1), ('fÃ¼r', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "fp = dict(Counter([x[0] for x in result2[1]]))\n",
    "fp = sorted(fp.items(),key=lambda x:x[1],reverse=True)\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non_trouvÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1937', 5), ('Dr', 4), ('Bundesrath', 4), ('ZÃ¼rich', 4), ('Prinzen', 3), ('Hr', 2), ('UniversitÃ¤t', 2), ('EmmenbrÃ¼cke', 2), ('1923', 2), ('von', 2), ('1936', 2), ('Commodore', 1), ('NelsonsBelohnung', 1), ('ben', 1), ('Bompartschen', 1), ('Abukir', 1), ('Gibraltar', 1), ('Bengalen', 1), ('neuen', 1), ('Tippo', 1), ('das', 1), ('bis', 1), ('Goubot', 1), ('Andujar', 1), ('Baylen', 1), ('DÃ¼ponts', 1), ('heint', 1), ('Belgien', 1), ('BÃ¼rgermeister', 1), ('LÃ¼ttich', 1), ('Herrde', 1), ('Hrn', 1), ('TÃ¼rkei', 1), ('Gott', 1), ('Dresden', 1), ('EuropÃ¤ischen', 1), ('1819', 1), ('Aarauerschienen', 1), ('KantonsZÃ¼rich', 1), ('eidgenÃ¶ssischen', 1), ('Bern', 1), ('Speichergasse', 1), ('Berichterstatter', 1), ('Staat', 1), ('Bundesrathe', 1), ('FÃ¼hrer', 1), ('Komitee', 1), ('Konstantinopel', 1), ('Alliierten', 1), ('Oesterreich', 1), ('Verbandes', 1), ('deutschnationalen', 1), ('deutschen', 1), ('deutschradikalen', 1), ('JosephvonHabsburg', 1), ('Ackerbauministerium', 1), ('ammlung', 1), ('Niederlanden', 1), ('KÃ¼ttner', 1), ('Sachsen', 1), ('SociÃ©tÃ©', 1), ('Japan', 1), ('Dreysus', 1), ('Clavel', 1), ('Usines', 1), ('Deutschlandvon', 1), ('I', 1), ('Italienpflanzt', 1), ('Schweizliegt', 1), ('Schweden', 1), ('als', 1), ('Widnau', 1), ('Heerbrugg', 1), ('Rheintal', 1), ('FeldmÃ¼hle', 1), ('Rheinfelden', 1), ('Das', 1), ('1938', 1), ('19363', 1), ('Prof', 1), ('Fretx', 1), ('1930', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "fn = dict(Counter([x[0] for x in result2[2]]))\n",
    "fn = sorted(fn.items(),key=lambda x:x[1],reverse=True)\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_incorrect = pd.DataFrame({\"NER\":[x[0] for x in fp],\"Frequecy\":[x[1] for x in fp]})\n",
    "tf_non_trouvÃ© = pd.DataFrame({\"NER\":[x[0] for x in fn],\"Frequency\":[x[1] for x in fn]})\n",
    "\n",
    "\n",
    "html_string1 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Term Frequency for incorrect NER</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "html_string2 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Term Frequency for non correct NER</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "with open(\"rÃ©sultat/Model with train + Spacy Model/tfFP.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=tf_incorrect.to_html()))\n",
    "\n",
    "with open(\"rÃ©sultat/Model with train + Spacy Model/tfFN.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=tf_non_trouvÃ©.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Prinzen\" in set([x[0] for x in result2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ZÃ¼rich\" in set([x[0] for x in result2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZÃ¼rich exists both in VP and Faux Negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZÃ¼rich', 14029, 14035, 'LOC', ' gewidmet hat.(Verlag Gebr. Fretx, ZÃ¼rich, 1930)'] 1\n",
      "['ZÃ¼rich', 10097, 10103, 'LOC', ' das physikalische Institut in ZÃ¼rich. Ein solcheserheischt nicht nur'] 4\n"
     ]
    }
   ],
   "source": [
    "ZÃ¼rich_vp = list(filter(lambda x:x[0]==\"ZÃ¼rich\",result2[0]))\n",
    "ZÃ¼rich_fn = list(filter(lambda x:x[0]==\"ZÃ¼rich\",result2[2]))\n",
    "print(ZÃ¼rich_vp[0],len(ZÃ¼rich_vp))\n",
    "print(ZÃ¼rich_fn[0],len(ZÃ¼rich_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>ZÃ¼rich</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "tmp1 = [x[4] for x in ZÃ¼rich_vp]\n",
    "tmp2 = [x[4] for x in ZÃ¼rich_fn]\n",
    "a,b = len(tmp1),len(tmp2)\n",
    "if a<b:\n",
    "    for x in range(b-a):\n",
    "        tmp1.append('')\n",
    "else:\n",
    "    for x in range(a-b):\n",
    "        tmp2.append(\"\")\n",
    "    \n",
    "\n",
    "l1 = pd.DataFrame({\"Find\":tmp1,\"Not Find\":tmp2},index = [x for x in range(1,max(a,b)+1)])\n",
    "with open(\"rÃ©sultat/Model with train + Spacy Model/ZÃ¼rich.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string.format(table=l1.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

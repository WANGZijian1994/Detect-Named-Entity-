{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals,print_function\n",
    "\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "import spacy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch,compounding\n",
    "nlp_de1 = spacy.load(\"de_core_news_sm\")\n",
    "nlp_de2 = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frankreich.', ['Frankreich', 0, 10, 'loc']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"novo_train_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Frankreich.', {'entities': [(0, 10, 'LOC')]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_format_for_train(data):    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(1,len(data[i])):\n",
    "            tmp = data[i][j][-1]\n",
    "            if tmp == 'pers':\n",
    "                data[i][j][-1]=\"PER\"\n",
    "            elif tmp == 'org':\n",
    "                data[i][j][-1]='ORG'\n",
    "            elif tmp == 'loc':\n",
    "                data[i][j][-1]='LOC'\n",
    "            else:\n",
    "                data[i][j][-1]='MISC' \n",
    "    DATA = []\n",
    "    for i in range(len(data)):\n",
    "        values = [(x[1],x[2],x[3]) for x in data[i][1:]]\n",
    "        DATA.append((data[i][0],{\"entities\":values}))\n",
    "    return DATA\n",
    "TRAIN_DATA = spacy_format_for_train(train_data)\n",
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses, {'ner': 21901.252704107843}\n",
      "Losses, {'ner': 5575.999694520142}\n",
      "Losses, {'ner': 5339.034751672356}\n",
      "Losses, {'ner': 4942.349053597078}\n",
      "Losses, {'ner': 4680.652463650156}\n",
      "Losses, {'ner': 4660.858366197004}\n",
      "Losses, {'ner': 4230.358784696698}\n",
      "Losses, {'ner': 4125.303089174908}\n",
      "Losses, {'ner': 3803.2817659351276}\n",
      "Losses, {'ner': 3809.411183443386}\n",
      "Losses, {'ner': 3493.343972873814}\n",
      "Losses, {'ner': 3603.288161922901}\n",
      "Losses, {'ner': 3420.027510996748}\n",
      "Losses, {'ner': 3299.4164641096067}\n",
      "Losses, {'ner': 3260.965101563066}\n",
      "Losses, {'ner': 3103.8086719831554}\n",
      "Losses, {'ner': 3102.76538397102}\n",
      "Losses, {'ner': 2945.934652855336}\n",
      "Losses, {'ner': 2729.9249583415403}\n",
      "Losses, {'ner': 3000.407493845226}\n",
      "Losses, {'ner': 2719.1636139813672}\n",
      "Losses, {'ner': 2567.027094748677}\n",
      "Losses, {'ner': 2579.592337316328}\n",
      "Losses, {'ner': 2529.518233144954}\n",
      "Losses, {'ner': 2421.1188388117816}\n",
      "Losses, {'ner': 2635.7878836361465}\n",
      "Losses, {'ner': 2338.217444127071}\n",
      "Losses, {'ner': 2348.42946402892}\n",
      "Losses, {'ner': 2155.1328641731366}\n",
      "Losses, {'ner': 2345.9553509682855}\n",
      "Losses, {'ner': 2024.0991188853263}\n",
      "Losses, {'ner': 2039.3293948910746}\n",
      "Losses, {'ner': 2055.3778795956337}\n",
      "Losses, {'ner': 2009.7534191298066}\n",
      "Losses, {'ner': 1832.4508412399082}\n",
      "Losses, {'ner': 1810.663912651171}\n",
      "Losses, {'ner': 2080.882484433571}\n",
      "Losses, {'ner': 1713.1587957590987}\n",
      "Losses, {'ner': 1670.0312964040268}\n",
      "Losses, {'ner': 1650.462245956765}\n",
      "Losses, {'ner': 1542.3936202035557}\n",
      "Losses, {'ner': 1488.6856058480244}\n",
      "Losses, {'ner': 1427.2216885268674}\n",
      "Losses, {'ner': 1476.5317635785627}\n",
      "Losses, {'ner': 1347.0567960737653}\n",
      "Losses, {'ner': 1390.1639847096833}\n",
      "Losses, {'ner': 1256.2252624117477}\n",
      "Losses, {'ner': 1241.2312478029216}\n",
      "Losses, {'ner': 1222.3505397954784}\n",
      "Losses, {'ner': 1200.4871193167946}\n",
      "Losses, {'ner': 1207.8744752972755}\n",
      "Losses, {'ner': 1251.1822981586606}\n",
      "Losses, {'ner': 1158.6521507179534}\n",
      "Losses, {'ner': 1121.4706864444001}\n",
      "Losses, {'ner': 1058.265087748715}\n",
      "Losses, {'ner': 1069.2318073959277}\n",
      "Losses, {'ner': 1039.7791164363348}\n",
      "Losses, {'ner': 1079.2166871329468}\n",
      "Losses, {'ner': 1129.4254070741954}\n",
      "Losses, {'ner': 1091.5723116127936}\n",
      "Losses, {'ner': 957.8638754336364}\n",
      "Losses, {'ner': 960.074616814145}\n",
      "Losses, {'ner': 879.3361853242712}\n",
      "Losses, {'ner': 890.8429293587451}\n",
      "Losses, {'ner': 913.7527836924485}\n",
      "Losses, {'ner': 863.4118464010517}\n",
      "Losses, {'ner': 798.9957643652394}\n",
      "Losses, {'ner': 854.5483532262182}\n",
      "Losses, {'ner': 835.6574901357717}\n",
      "Losses, {'ner': 776.121276619108}\n",
      "Losses, {'ner': 766.2581222343598}\n",
      "Losses, {'ner': 737.8265279727364}\n",
      "Losses, {'ner': 772.9373772324789}\n",
      "Losses, {'ner': 752.987630528976}\n",
      "Losses, {'ner': 715.3303961047851}\n",
      "Losses, {'ner': 662.7815802491539}\n",
      "Losses, {'ner': 700.236708130139}\n",
      "Losses, {'ner': 752.1726764748256}\n",
      "Losses, {'ner': 710.7095921385434}\n",
      "Losses, {'ner': 648.7923905239717}\n",
      "Losses, {'ner': 705.7099434265696}\n",
      "Losses, {'ner': 681.5635727503457}\n",
      "Losses, {'ner': 702.0967778921065}\n",
      "Losses, {'ner': 819.0567729172928}\n",
      "Losses, {'ner': 766.9633737528316}\n",
      "Losses, {'ner': 647.647320847943}\n",
      "Losses, {'ner': 680.6306494977144}\n",
      "Losses, {'ner': 648.7753535348479}\n",
      "Losses, {'ner': 794.7536572974859}\n",
      "Losses, {'ner': 826.1138640629334}\n",
      "Losses, {'ner': 657.5533676546604}\n",
      "Losses, {'ner': 608.7915463489768}\n",
      "Losses, {'ner': 602.6585624665399}\n",
      "Losses, {'ner': 623.0189360643386}\n",
      "Losses, {'ner': 581.759848145101}\n",
      "Losses, {'ner': 546.3832669426423}\n",
      "Losses, {'ner': 565.9627530247599}\n",
      "Losses, {'ner': 569.0821737392862}\n",
      "Losses, {'ner': 533.8109390591317}\n",
      "Losses, {'ner': 540.3698442758587}\n",
      "Losses, {'ner': 524.9270747152497}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def create_model(output_dir,n_iter,TRAIN_DATA):\n",
    "    nlp = spacy.blank(\"de\")\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner,last=True)\n",
    "    \n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch （speed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "create_model(\"/home/zijian/ZijianStageNER/Novo_Models/Create/\",101,TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch （speed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this new model with dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"novo_dev_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "\n",
    "DEV_DATA = spacy_format_for_train(dev)\n",
    "for i in range(len(DEV_DATA)):\n",
    "    DEV_DATA[i] = (DEV_DATA[i][0],DEV_DATA[i][1][\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(Condeer.] Petersburg den 18. Dec. Se.russisch- kaiserl. Majestät haben dem Prinzen vonCondé bey dessen Ankunft in Petersburg den St.Andreas-Orden u. den Maltheser Ritterorden in Polen zu ertheilen, und ihn mit einem prächtigen,völlig meublirten Palais in Petersburg zu beschenken geruhet. Das aus 3 Infanterie- und 2 Kavallerie-Regimentern bestehende Corps des Prinzen vonCondé, welches in kaiserliche Dienste genommenworden, ist nun nach Wladimir, Luzk und Kowelin Quartier verlegt. Das ganze Corps wird unterbestandiger Inspection des Prinzen von Condéstehen. Se. kaiserl. Majestät haben ihn zum Chef desadelichen Infanterie-Regiments, und den Duc deBerry zum Chef des adelichen Kavallerie-Regiments ernannt. Als der Prinz in seinen Pallasttrat, fand er daselbst bereits Leute mit seiner Libréevor, auch Carossen mit seinem Wappen. Der Prinzwar in Verlegenheit an welcher Stelle er eigentlichdas Zeichen des St. Andreas-Ordens tragen sollte.Der Kaiser antwortete ihm: Er möchte es mit denInsignien des hl. Geist-Ordens auf derselben Linietragen, und hieng ihm den Orden um.',\n",
       " [(11, 21, 'LOC'),\n",
       "  (76, 83, 'PER'),\n",
       "  (115, 125, 'LOC'),\n",
       "  (179, 184, 'LOC'),\n",
       "  (256, 266, 'LOC'),\n",
       "  (362, 369, 'PER'),\n",
       "  (440, 448, 'LOC'),\n",
       "  (450, 454, 'LOC'),\n",
       "  (459, 466, 'LOC'),\n",
       "  (538, 545, 'PER'),\n",
       "  (647, 650, 'PER')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model,data):\n",
    "    scorer = Scorer()\n",
    "    for text,annot in data:\n",
    "        doc_gold_text = ner_model.make_doc(text)\n",
    "        gold = GoldParse(doc_gold_text,entities=annot)\n",
    "        pred_value = ner_model(text)\n",
    "        scorer.score(pred_value,gold)\n",
    "    return scorer.scores\n",
    "\n",
    "\n",
    "path = \"/home/zijian/ZijianStageNER/Novo_Models/Create\"\n",
    "nlp2 = spacy.load(path)\n",
    "results = evaluate(nlp2,DEV_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train by updating the model in spacy de_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch （speed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precison</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model only with my train_data</th>\n",
       "      <td>64.814815</td>\n",
       "      <td>53.030303</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model with data TigerCorpus and my train data</th>\n",
       "      <td>70.909091</td>\n",
       "      <td>59.090909</td>\n",
       "      <td>64.462810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                precison     recall         f1\n",
       "Model only with my train_data                  64.814815  53.030303  58.333333\n",
       "Model with data TigerCorpus and my train data  70.909091  59.090909  64.462810"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/zijian/ZijianStageNER/RetrainModels/\"\n",
    "nlp2 = spacy.load(path)\n",
    "results2 = evaluate(nlp2,DEV_DATA)\n",
    "\n",
    "res = pd.DataFrame({\"precison\":[results['ents_p'],results2['ents_p']],\"recall\":[results['ents_r'],results2['ents_r']],\"f1\":[results['ents_f'],results2['ents_f']]},index=[\"Model only with my train_data\",\"Model with data TigerCorpus and my train data\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What is the difference? Model 1 could detect what? Model 2 could detect what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data_Model = spacy.load(\"/home/zijian/ZijianStageNER/Novo_Models/Create/\")\n",
    "SM_plus_Train_DATA_Model = spacy.load(\"/home/zijian/ZijianStageNER/RetrainModels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trouvés_corrects = []\n",
    "trouvés_incorrect = []\n",
    "non_trouvé_mais_correct = []\n",
    "res1 = []\n",
    "correction1 = []\n",
    "for i in range(len(dev)):\n",
    "    doc = SM_plus_Train_DATA_Model(dev[i][0])\n",
    "    correction1.append(dev[i][1:])\n",
    "    tmp = []\n",
    "    for ent in doc.ents:\n",
    "        tmp.append([ent.text,ent.start_char,ent.end_char,ent.label_])\n",
    "    res1.append(tmp)\n",
    "for i in range(len(res1)):\n",
    "    l1 = [(x[0],x[1],x[2],x[3]) for x in res1[i]]\n",
    "    l2 = [(x[0],x[1],x[2],x[3]) for x in correction1[i]]\n",
    "    trouvés_corrects += list(set(l1) & set(l2))\n",
    "    trouvés_incorrect += list(set(l1) - set(l2))\n",
    "    non_trouvé_mais_correct += list(set(l2)-set(l1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(text,i,j):\n",
    "    left,right = i,j\n",
    "    count = 0\n",
    "    if left > 0:\n",
    "        while count < 5 and left > 0:\n",
    "            left-=1\n",
    "            if text[left]==' ':\n",
    "                count+=1\n",
    "    count = 0\n",
    "    while count < 5 and right < len(text):\n",
    "        right += 1\n",
    "        if text[right]==' ':\n",
    "            count += 1\n",
    "    return text[left+1:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rußland', 0, 7, 'LOC')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trouvés_corrects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp = max([len(trouvés_corrects),len(trouvés_incorrect),len(non_trouvé_mais_correct)])\n",
    "for i in range(len(trouvés_corrects),tmp):\n",
    "    trouvés_corrects.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(trouvés_incorrect),tmp):\n",
    "    trouvés_incorrect.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(non_trouvé_mais_correct),tmp):\n",
    "    non_trouvé_mais_correct.append((\"\",\"\",\"\",\"\"))\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"Trouvé corrects\":[(x[0],x[3]) for x in trouvés_corrects],\"Trouvés incorrects\":[(x[0],x[3]) for x in trouvés_incorrect],\"non trouvé mais correct\":[(x[0],x[3]) for x in non_trouvé_mais_correct]},index = [x for x in range(1,tmp+1)])\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Model only with train data</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "with open(\"Model with train data and spacy data.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string.format(table=d.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP and their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trouvés_incorrect = [(x[0],x[1],x[2],x[3],context(x[0],x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp = max([len(trouvés_corrects),len(trouvés_incorrect),len(non_trouvé_mais_correct)])\n",
    "for i in range(len(trouvés_corrects),tmp):\n",
    "    trouvés_corrects.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(trouvés_incorrect),tmp):\n",
    "    trouvés_incorrect.append((\"\",\"\",\"\",\"\"))\n",
    "for i in range(len(non_trouvé_mais_correct),tmp):\n",
    "    non_trouvé_mais_correct.append((\"\",\"\",\"\",\"\"))\n",
    "\n",
    "\n",
    "d = pd.DataFrame({\"Trouvé corrects\":[(x[0],x[3]) for x in trouvés_corrects],\"Trouvés incorrects\":[(x[0],x[3]) for x in trouvés_incorrect],\"non trouvé mais correct\":[(x[0],x[3]) for x in non_trouvé_mais_correct]},index = [x for x in range(1,tmp+1)])\n",
    "\n",
    "html_string = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Model only with train data</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

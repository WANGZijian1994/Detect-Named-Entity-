{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals,print_function\n",
    "\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "import spacy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch,compounding\n",
    "nlp_de1 = spacy.load(\"de_core_news_sm\")\n",
    "nlp_de2 = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frankreich.', ['Frankreich', 0, 10, 'loc']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"novo_train_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Frankreich.', {'entities': [(0, 10, 'LOC')]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_format_for_train(data):    \n",
    "    for i in range(len(data)):\n",
    "        for j in range(1,len(data[i])):\n",
    "            tmp = data[i][j][-1]\n",
    "            if tmp == 'pers':\n",
    "                data[i][j][-1]=\"PER\"\n",
    "            elif tmp == 'org':\n",
    "                data[i][j][-1]='ORG'\n",
    "            elif tmp == 'loc':\n",
    "                data[i][j][-1]='LOC'\n",
    "            else:\n",
    "                data[i][j][-1]='MISC' \n",
    "    DATA = []\n",
    "    for i in range(len(data)):\n",
    "        values = [(x[1],x[2],x[3]) for x in data[i][1:]]\n",
    "        DATA.append((data[i][0],{\"entities\":values}))\n",
    "    return DATA\n",
    "TRAIN_DATA = spacy_format_for_train(train_data)\n",
    "TRAIN_DATA[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses, {'ner': 21901.252704107843}\n",
      "Losses, {'ner': 5575.999694520142}\n",
      "Losses, {'ner': 5339.034751672356}\n",
      "Losses, {'ner': 4942.349053597078}\n",
      "Losses, {'ner': 4680.652463650156}\n",
      "Losses, {'ner': 4660.858366197004}\n",
      "Losses, {'ner': 4230.358784696698}\n",
      "Losses, {'ner': 4125.303089174908}\n",
      "Losses, {'ner': 3803.2817659351276}\n",
      "Losses, {'ner': 3809.411183443386}\n",
      "Losses, {'ner': 3493.343972873814}\n",
      "Losses, {'ner': 3603.288161922901}\n",
      "Losses, {'ner': 3420.027510996748}\n",
      "Losses, {'ner': 3299.4164641096067}\n",
      "Losses, {'ner': 3260.965101563066}\n",
      "Losses, {'ner': 3103.8086719831554}\n",
      "Losses, {'ner': 3102.76538397102}\n",
      "Losses, {'ner': 2945.934652855336}\n",
      "Losses, {'ner': 2729.9249583415403}\n",
      "Losses, {'ner': 3000.407493845226}\n",
      "Losses, {'ner': 2719.1636139813672}\n",
      "Losses, {'ner': 2567.027094748677}\n",
      "Losses, {'ner': 2579.592337316328}\n",
      "Losses, {'ner': 2529.518233144954}\n",
      "Losses, {'ner': 2421.1188388117816}\n",
      "Losses, {'ner': 2635.7878836361465}\n",
      "Losses, {'ner': 2338.217444127071}\n",
      "Losses, {'ner': 2348.42946402892}\n",
      "Losses, {'ner': 2155.1328641731366}\n",
      "Losses, {'ner': 2345.9553509682855}\n",
      "Losses, {'ner': 2024.0991188853263}\n",
      "Losses, {'ner': 2039.3293948910746}\n",
      "Losses, {'ner': 2055.3778795956337}\n",
      "Losses, {'ner': 2009.7534191298066}\n",
      "Losses, {'ner': 1832.4508412399082}\n",
      "Losses, {'ner': 1810.663912651171}\n",
      "Losses, {'ner': 2080.882484433571}\n",
      "Losses, {'ner': 1713.1587957590987}\n",
      "Losses, {'ner': 1670.0312964040268}\n",
      "Losses, {'ner': 1650.462245956765}\n",
      "Losses, {'ner': 1542.3936202035557}\n",
      "Losses, {'ner': 1488.6856058480244}\n",
      "Losses, {'ner': 1427.2216885268674}\n",
      "Losses, {'ner': 1476.5317635785627}\n",
      "Losses, {'ner': 1347.0567960737653}\n",
      "Losses, {'ner': 1390.1639847096833}\n",
      "Losses, {'ner': 1256.2252624117477}\n",
      "Losses, {'ner': 1241.2312478029216}\n",
      "Losses, {'ner': 1222.3505397954784}\n",
      "Losses, {'ner': 1200.4871193167946}\n",
      "Losses, {'ner': 1207.8744752972755}\n",
      "Losses, {'ner': 1251.1822981586606}\n",
      "Losses, {'ner': 1158.6521507179534}\n",
      "Losses, {'ner': 1121.4706864444001}\n",
      "Losses, {'ner': 1058.265087748715}\n",
      "Losses, {'ner': 1069.2318073959277}\n",
      "Losses, {'ner': 1039.7791164363348}\n",
      "Losses, {'ner': 1079.2166871329468}\n",
      "Losses, {'ner': 1129.4254070741954}\n",
      "Losses, {'ner': 1091.5723116127936}\n",
      "Losses, {'ner': 957.8638754336364}\n",
      "Losses, {'ner': 960.074616814145}\n",
      "Losses, {'ner': 879.3361853242712}\n",
      "Losses, {'ner': 890.8429293587451}\n",
      "Losses, {'ner': 913.7527836924485}\n",
      "Losses, {'ner': 863.4118464010517}\n",
      "Losses, {'ner': 798.9957643652394}\n",
      "Losses, {'ner': 854.5483532262182}\n",
      "Losses, {'ner': 835.6574901357717}\n",
      "Losses, {'ner': 776.121276619108}\n",
      "Losses, {'ner': 766.2581222343598}\n",
      "Losses, {'ner': 737.8265279727364}\n",
      "Losses, {'ner': 772.9373772324789}\n",
      "Losses, {'ner': 752.987630528976}\n",
      "Losses, {'ner': 715.3303961047851}\n",
      "Losses, {'ner': 662.7815802491539}\n",
      "Losses, {'ner': 700.236708130139}\n",
      "Losses, {'ner': 752.1726764748256}\n",
      "Losses, {'ner': 710.7095921385434}\n",
      "Losses, {'ner': 648.7923905239717}\n",
      "Losses, {'ner': 705.7099434265696}\n",
      "Losses, {'ner': 681.5635727503457}\n",
      "Losses, {'ner': 702.0967778921065}\n",
      "Losses, {'ner': 819.0567729172928}\n",
      "Losses, {'ner': 766.9633737528316}\n",
      "Losses, {'ner': 647.647320847943}\n",
      "Losses, {'ner': 680.6306494977144}\n",
      "Losses, {'ner': 648.7753535348479}\n",
      "Losses, {'ner': 794.7536572974859}\n",
      "Losses, {'ner': 826.1138640629334}\n",
      "Losses, {'ner': 657.5533676546604}\n",
      "Losses, {'ner': 608.7915463489768}\n",
      "Losses, {'ner': 602.6585624665399}\n",
      "Losses, {'ner': 623.0189360643386}\n",
      "Losses, {'ner': 581.759848145101}\n",
      "Losses, {'ner': 546.3832669426423}\n",
      "Losses, {'ner': 565.9627530247599}\n",
      "Losses, {'ner': 569.0821737392862}\n",
      "Losses, {'ner': 533.8109390591317}\n",
      "Losses, {'ner': 540.3698442758587}\n",
      "Losses, {'ner': 524.9270747152497}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def create_model(output_dir,n_iter,TRAIN_DATA):\n",
    "    nlp = spacy.blank(\"de\")\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner,last=True)\n",
    "    \n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch （speed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "create_model(\"/home/zijian/ZijianStageNER/Novo_Models/Create/\",101,TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch （speed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this new model with dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"novo_dev_de.json\",'r',encoding=\"utf-8\")as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "\n",
    "DEV_DATA = spacy_format_for_train(dev)\n",
    "for i in range(len(DEV_DATA)):\n",
    "    DEV_DATA[i] = (DEV_DATA[i][0],DEV_DATA[i][1][\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('(Condeer.] Petersburg den 18. Dec. Se.russisch- kaiserl. Majestät haben dem Prinzen vonCondé bey dessen Ankunft in Petersburg den St.Andreas-Orden u. den Maltheser Ritterorden in Polen zu ertheilen, und ihn mit einem prächtigen,völlig meublirten Palais in Petersburg zu beschenken geruhet. Das aus 3 Infanterie- und 2 Kavallerie-Regimentern bestehende Corps des Prinzen vonCondé, welches in kaiserliche Dienste genommenworden, ist nun nach Wladimir, Luzk und Kowelin Quartier verlegt. Das ganze Corps wird unterbestandiger Inspection des Prinzen von Condéstehen. Se. kaiserl. Majestät haben ihn zum Chef desadelichen Infanterie-Regiments, und den Duc deBerry zum Chef des adelichen Kavallerie-Regiments ernannt. Als der Prinz in seinen Pallasttrat, fand er daselbst bereits Leute mit seiner Libréevor, auch Carossen mit seinem Wappen. Der Prinzwar in Verlegenheit an welcher Stelle er eigentlichdas Zeichen des St. Andreas-Ordens tragen sollte.Der Kaiser antwortete ihm: Er möchte es mit denInsignien des hl. Geist-Ordens auf derselben Linietragen, und hieng ihm den Orden um.',\n",
       " [(11, 21, 'LOC'),\n",
       "  (76, 83, 'PER'),\n",
       "  (115, 125, 'LOC'),\n",
       "  (179, 184, 'LOC'),\n",
       "  (256, 266, 'LOC'),\n",
       "  (362, 369, 'PER'),\n",
       "  (440, 448, 'LOC'),\n",
       "  (450, 454, 'LOC'),\n",
       "  (459, 466, 'LOC'),\n",
       "  (538, 545, 'PER'),\n",
       "  (647, 650, 'PER')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model,data):\n",
    "    scorer = Scorer()\n",
    "    for text,annot in data:\n",
    "        doc_gold_text = ner_model.make_doc(text)\n",
    "        gold = GoldParse(doc_gold_text,entities=annot)\n",
    "        pred_value = ner_model(text)\n",
    "        scorer.score(pred_value,gold)\n",
    "    return scorer.scores\n",
    "\n",
    "\n",
    "#path = \"/home/zijian/ZijianStageNER/Novo_Models/Create\"\n",
    "#nlp2 = spacy.load(path)\n",
    "#results = evaluate(nlp2,DEV_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train by updating the model in spacy de_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(TRAIN_DATA,model,output_dir,n_iter):\n",
    "    nlp = nlp_de1\n",
    "    \n",
    "    # get ner pipelines for this model so that we can modify labels\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for x,y in TRAIN_DATA:\n",
    "        for ent in y.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        \n",
    "    # train ner but not others\n",
    "    pipe_exceptions = set([\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\",category=UserWarning,module=\"spacy\")\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch （speed up for training)\n",
    "            batches = minibatch(TRAIN_DATA,size=compounding(4.0,32.0,1.001))\n",
    "            for batch in batches:\n",
    "                texts,annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,\n",
    "                    annotations,\n",
    "                    drop=0.5,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses,\",losses)\n",
    "    \n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "training(TRAIN_DATA,\"de_core_news_sm\",\"/home/zijian/ZijianStageNER/RetrainModels/\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precison</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model only with my train_data</th>\n",
       "      <td>64.814815</td>\n",
       "      <td>53.030303</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model with data TigerCorpus and my train data</th>\n",
       "      <td>70.909091</td>\n",
       "      <td>59.090909</td>\n",
       "      <td>64.462810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                precison     recall         f1\n",
       "Model only with my train_data                  64.814815  53.030303  58.333333\n",
       "Model with data TigerCorpus and my train data  70.909091  59.090909  64.462810"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/zijian/ZijianStageNER/RetrainModels/\"\n",
    "nlp2 = spacy.load(path)\n",
    "results2 = evaluate(nlp2,DEV_DATA)\n",
    "\n",
    "res = pd.DataFrame({\"precison\":[results['ents_p'],results2['ents_p']],\"recall\":[results['ents_r'],results2['ents_r']],\"f1\":[results['ents_f'],results2['ents_f']]},index=[\"Model only with my train_data\",\"Model with data TigerCorpus and my train data\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So What is the difference? Model 1 could detect what? Model 2 could detect what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data_Model = spacy.load(\"ModelsRetrained/Novo_Models/Create/\")\n",
    "SM_plus_Train_DATA_Model = spacy.load(\"ModelsRetrained/Novo_Models/UpdateModel1/RetrainModels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40   40\n"
     ]
    }
   ],
   "source": [
    "def context(text,i,j):\n",
    "    left,right = i,j\n",
    "    count = 0\n",
    "    if left > 0:\n",
    "        while count < 5 and left > 0:\n",
    "            left-=1\n",
    "            if text[left]==' ':\n",
    "                count+=1\n",
    "    count = 0\n",
    "    while count < 5 and right+1 < len(text):\n",
    "        right += 1\n",
    "        if text[right]==' ':\n",
    "            count += 1\n",
    "    return text[left:right]\n",
    "\n",
    "def consulter(model,dev):\n",
    "    results = []\n",
    "    for i in range(len(dev)):\n",
    "        doc = model(dev[i][0])\n",
    "        tmp = []\n",
    "        for ent in doc.ents:\n",
    "            tmp.append([ent.text,ent.start_char,ent.end_char,ent.label_,context(dev[i][0],ent.start_char,ent.end_char)])\n",
    "        results.append(tmp) \n",
    "    return results\n",
    "\n",
    "# Model with train_de.json :\n",
    "res1 = consulter(Train_Data_Model,dev)\n",
    "\n",
    "# Model with train_de.json + sm:\n",
    "res2 = consulter(SM_plus_Train_DATA_Model,dev)\n",
    "\n",
    "print(len(res1),\" \",len(res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Petersburg', 11, 21, 'LOC', '(Condeer.] Petersburg den 18. Dec. Se.russisch- kaiserl.'], ['Prinzen', 76, 83, 'PER', ' kaiserl. Majestät haben dem Prinzen vonCondé bey dessen Ankunft in'], ['Petersburg', 115, 125, 'LOC', ' bey dessen Ankunft in Petersburg den St.Andreas-Orden u. den Maltheser'], ['Polen', 179, 184, 'LOC', ' den Maltheser Ritterorden in Polen zu ertheilen, und ihn mit'], ['Petersburg', 256, 266, 'LOC', ' prächtigen,völlig meublirten Palais in Petersburg zu beschenken geruhet. Das aus'], ['Prinzen', 362, 369, 'PER', ' Kavallerie-Regimentern bestehende Corps des Prinzen vonCondé, welches in kaiserliche Dienste'], ['Wladimir', 440, 448, 'LOC', ' genommenworden, ist nun nach Wladimir, Luzk und Kowelin Quartier'], ['Luzk', 450, 454, 'LOC', ' ist nun nach Wladimir, Luzk und Kowelin Quartier verlegt. Das'], ['Kowelin', 459, 466, 'LOC', ' nach Wladimir, Luzk und Kowelin Quartier verlegt. Das ganze Corps'], ['Prinzen', 538, 545, 'PER', ' wird unterbestandiger Inspection des Prinzen von Condéstehen. Se. kaiserl. Majestät'], ['Duc', 647, 650, 'PER', ' desadelichen Infanterie-Regiments, und den Duc deBerry zum Chef des adelichen']]\n"
     ]
    }
   ],
   "source": [
    "def correction():\n",
    "    correction = []\n",
    "    for i in range(len(dev)):\n",
    "        text = dev[i][0]\n",
    "        tmp = []\n",
    "        for x in dev[i][1:]:\n",
    "            tmp.append([x[0],x[1],x[2],x[3],context(text,x[1],x[2])])\n",
    "        correction.append(tmp[:])\n",
    "    return correction\n",
    "correction = correction()\n",
    "print(correction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(res,correction):        \n",
    "    VP = []\n",
    "    FP = []\n",
    "    VN = [] \n",
    "    i,j = 0,0\n",
    "    for x in range(len(res)):\n",
    "        a = res[x]\n",
    "        b = correction[x]\n",
    "        while i<len(a) and j<len(b):\n",
    "            l1,r1 = a[i][1],a[i][2]\n",
    "            l2,r2 = b[j][1],b[j][2]\n",
    "            if l1>r2:\n",
    "                VN.append(b[j])\n",
    "                j+=1\n",
    "            elif r1 < l2:\n",
    "                FP.append(a[i])\n",
    "                i+=1\n",
    "            elif l1==l2 and r1==r2:\n",
    "                if a[i][3]!=b[j][3]:\n",
    "                    a[i][3] += \" but correct : ({})\".format(b[j][3])\n",
    "                    FP.append(a[i])\n",
    "                else:\n",
    "                    VP.append(a[i])\n",
    "                i+=1\n",
    "                j+=1\n",
    "            else:\n",
    "                if a[i][0] in b[j][4]:\n",
    "                    if a[i][3]!=b[j][3]:\n",
    "                        a[i][3] += \" but correct : ({})\".format(b[j][3])\n",
    "                        FP.append(a[i])\n",
    "                    else:\n",
    "                        VP.append(a[i])\n",
    "                else:\n",
    "                    FP.append(a[i])\n",
    "                i+=1\n",
    "                j+=1  \n",
    "        while i<len(a):\n",
    "            FP.append(a[i])\n",
    "            i+=1\n",
    "        while j<len(b):\n",
    "            VN.append(b[j])\n",
    "            j+=1\n",
    "    return [VP,FP,VN]\n",
    "\n",
    "result1 = evaluate(res1,correction)\n",
    "result2 = evaluate(res2,correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d1 = pd.DataFrame({\"NER\":[x[0] for x in result1[1]],\"Type\":[x[3] for x in result1[1]],\"Context\":[x[4] for x in result1[1]]},index = [x for x in range(1,len(result1[1])+1)])\n",
    "d2 = pd.DataFrame({\"NER\":[x[0] for x in result1[2]],\"Type\":[x[3] for x in result1[2]],\"Context\":[x[4] for x in result1[2]]},index = [x for x in range(1,len(result1[2])+1)])\n",
    "\n",
    "d3 = pd.DataFrame({\"NER\":[x[0] for x in result2[1]],\"Type\":[x[3] for x in result2[1]],\"Context\":[x[4] for x in result2[1]]},index = [x for x in range(1,len(result2[1])+1)])\n",
    "d4 = pd.DataFrame({\"NER\":[x[0] for x in result2[2]],\"Type\":[x[3] for x in result2[2]],\"Context\":[x[4] for x in result2[2]]},index = [x for x in range(1,len(result2[2])+1)])\n",
    "\n",
    "\n",
    "html_string1 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Faux Postif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "html_string2 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Vrai Négatif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "with open(\"résultat/Model only with train/Faux Positif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=d1.to_html()))\n",
    "with open(\"résultat/Model only with train/Vrai Négatif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string2.format(table=d2.to_html()))\n",
    "with open(\"résultat/Model with train + Spacy Model/Faux Positif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=d3.to_html()))\n",
    "with open(\"résultat/Model with train + Spacy Model/Vrai Négatif.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string2.format(table=d4.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for the results\n",
    "\n",
    "\n",
    "#### Type of NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(result2,typeLabel):\n",
    "    total = []\n",
    "    trouvé = list(filter(lambda x:x[3]==typeLabel,result2[0]))\n",
    "    incorrect = list(filter(lambda x:x[3]==typeLabel,result2[1]))\n",
    "    non_trouvé = list(filter(lambda x:x[3]==typeLabel,result2[2]))\n",
    "    total+=(trouvé+incorrect+non_trouvé)\n",
    "    p = len(trouvé)/(len(trouvé)+len(incorrect))*100\n",
    "    r = len(trouvé)/(len(non_trouvé)+len(trouvé))*100\n",
    "    if p+r!=0:\n",
    "        f = 2*p*r/(p+r)\n",
    "    else:\n",
    "        f = 0.0\n",
    "    return [p,r,f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71.42857142857143, 48.38709677419355, 57.6923076923077]\n"
     ]
    }
   ],
   "source": [
    "loc = stats(result2,\"LOC\")\n",
    "print(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. PERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.83333333333333, 25.0, 32.35294117647059]\n"
     ]
    }
   ],
   "source": [
    "pers = stats(result2,\"PER\")\n",
    "print(pers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "orgs = stats(result2,\"ORG\")\n",
    "print(orgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.0, 9.523809523809524, 15.384615384615385]\n"
     ]
    }
   ],
   "source": [
    "misc = stats(result2,\"MISC\")\n",
    "print(misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miscellaneous entities, e.g. events, nationalities, products or works of art'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"MISC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Précision</th>\n",
       "      <th>Rappel</th>\n",
       "      <th>F-Mesure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>71.428571</td>\n",
       "      <td>48.387097</td>\n",
       "      <td>57.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Person</th>\n",
       "      <td>45.833333</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>32.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organisation</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miscellaneous entities</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>15.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Précision     Rappel   F-Mesure\n",
       "Location                71.428571  48.387097  57.692308\n",
       "Person                  45.833333  25.000000  32.352941\n",
       "Organisation             0.000000   0.000000   0.000000\n",
       "Miscellaneous entities  40.000000   9.523810  15.384615"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.DataFrame({\"Précision\":[loc[0],pers[0],orgs[0],misc[0]],\"Rappel\":[loc[1],pers[1],orgs[1],misc[1]],'F-Mesure':[loc[2],pers[2],orgs[2],misc[2]]},index = [\"Location\",\"Person\",\"Organisation\",\"Miscellaneous entities\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string1 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Faux Postif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "html_string2 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Vrai Négatif</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "with open(\"résultat/Model with train + Spacy Model/Chaque_type.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=res.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe every NER not finded or finded but not correct\n",
    "\n",
    "##### Incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lausanne', 2), ('Petersburg', 1), ('St', 1), ('Kowelin', 1), ('Se', 1), ('Duc', 1), ('consolidirten3', 1), ('Landein', 1), ('Courier', 1), ('Lord', 1), ('Kühnheitgehabt', 1), ('Löwen', 1), ('Asselt', 1), ('Batavische', 1), ('Schuldenscheine', 1), ('Pest', 1), ('Zollvereinsstaatenzu', 1), ('Times', 1), ('Vizeadmiral', 1), ('Henry', 1), ('Talaat', 1), ('Volkan', 1), ('Graf', 1), ('Baron', 1), ('Ritter', 1), ('am', 1), ('Schweiz', 1), ('Rhodiaseta', 1), ('Celanaise', 1), ('British', 1), ('Bemberg', 1), ('Pemberg', 1), ('Kontinuierlich', 1), ('Eleinkeram', 1), ('Viscose', 1), ('Azetat', 1), ('für', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "fp = dict(Counter([x[0] for x in result2[1]]))\n",
    "fp = sorted(fp.items(),key=lambda x:x[1],reverse=True)\n",
    "print(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non_trouvé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1937', 5), ('Dr', 4), ('Bundesrath', 4), ('Zürich', 4), ('Prinzen', 3), ('Hr', 2), ('Universität', 2), ('Emmenbrücke', 2), ('1923', 2), ('von', 2), ('1936', 2), ('Commodore', 1), ('NelsonsBelohnung', 1), ('ben', 1), ('Bompartschen', 1), ('Abukir', 1), ('Gibraltar', 1), ('Bengalen', 1), ('neuen', 1), ('Tippo', 1), ('das', 1), ('bis', 1), ('Goubot', 1), ('Andujar', 1), ('Baylen', 1), ('Düponts', 1), ('heint', 1), ('Belgien', 1), ('Bürgermeister', 1), ('Lüttich', 1), ('Herrde', 1), ('Hrn', 1), ('Türkei', 1), ('Gott', 1), ('Dresden', 1), ('Europäischen', 1), ('1819', 1), ('Aarauerschienen', 1), ('KantonsZürich', 1), ('eidgenössischen', 1), ('Bern', 1), ('Speichergasse', 1), ('Berichterstatter', 1), ('Staat', 1), ('Bundesrathe', 1), ('Führer', 1), ('Komitee', 1), ('Konstantinopel', 1), ('Alliierten', 1), ('Oesterreich', 1), ('Verbandes', 1), ('deutschnationalen', 1), ('deutschen', 1), ('deutschradikalen', 1), ('JosephvonHabsburg', 1), ('Ackerbauministerium', 1), ('ammlung', 1), ('Niederlanden', 1), ('Küttner', 1), ('Sachsen', 1), ('Société', 1), ('Japan', 1), ('Dreysus', 1), ('Clavel', 1), ('Usines', 1), ('Deutschlandvon', 1), ('I', 1), ('Italienpflanzt', 1), ('Schweizliegt', 1), ('Schweden', 1), ('als', 1), ('Widnau', 1), ('Heerbrugg', 1), ('Rheintal', 1), ('Feldmühle', 1), ('Rheinfelden', 1), ('Das', 1), ('1938', 1), ('19363', 1), ('Prof', 1), ('Fretx', 1), ('1930', 1)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "fn = dict(Counter([x[0] for x in result2[2]]))\n",
    "fn = sorted(fn.items(),key=lambda x:x[1],reverse=True)\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_incorrect = pd.DataFrame({\"NER\":[x[0] for x in fp],\"Frequecy\":[x[1] for x in fp]})\n",
    "tf_non_trouvé = pd.DataFrame({\"NER\":[x[0] for x in fn],\"Frequency\":[x[1] for x in fn]})\n",
    "\n",
    "\n",
    "html_string1 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Term Frequency for incorrect NER</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "\n",
    "html_string2 = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Term Frequency for non correct NER</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "with open(\"résultat/Model with train + Spacy Model/tfFP.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=tf_incorrect.to_html()))\n",
    "\n",
    "with open(\"résultat/Model with train + Spacy Model/tfFN.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string1.format(table=tf_non_trouvé.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Prinzen\" in set([x[0] for x in result2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Zürich\" in set([x[0] for x in result2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zürich exists both in VP and Faux Negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zürich', 14029, 14035, 'LOC', ' gewidmet hat.(Verlag Gebr. Fretx, Zürich, 1930)'] 1\n",
      "['Zürich', 10097, 10103, 'LOC', ' das physikalische Institut in Zürich. Ein solcheserheischt nicht nur'] 4\n"
     ]
    }
   ],
   "source": [
    "Zürich_vp = list(filter(lambda x:x[0]==\"Zürich\",result2[0]))\n",
    "Zürich_fn = list(filter(lambda x:x[0]==\"Zürich\",result2[2]))\n",
    "print(Zürich_vp[0],len(Zürich_vp))\n",
    "print(Zürich_fn[0],len(Zürich_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = '''\n",
    "<html>\n",
    "  <meta charset=\"UTF-8\"></meta>\n",
    "  <head><title>Zürich</title></head>\n",
    "  <body>\n",
    "    {table}\n",
    "  </body>\n",
    "</html>.\n",
    "'''\n",
    "tmp1 = [x[4] for x in Zürich_vp]\n",
    "tmp2 = [x[4] for x in Zürich_fn]\n",
    "a,b = len(tmp1),len(tmp2)\n",
    "if a<b:\n",
    "    for x in range(b-a):\n",
    "        tmp1.append('')\n",
    "else:\n",
    "    for x in range(a-b):\n",
    "        tmp2.append(\"\")\n",
    "    \n",
    "\n",
    "l1 = pd.DataFrame({\"Find\":tmp1,\"Not Find\":tmp2},index = [x for x in range(1,max(a,b)+1)])\n",
    "with open(\"résultat/Model with train + Spacy Model/Zürich.html\",\"w\",encoding='utf-8') as f:\n",
    "    f.write(html_string.format(table=l1.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
